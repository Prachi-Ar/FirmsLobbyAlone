# -*- coding: utf-8 -*-
"""lobbyalone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/prachiarya0067/00dfd509cf47d9429095df7bf7f560fb/lobbyalone.ipynb
"""

#install packages
import pandas as pd

import requests

import io

import numpy as np

import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

#Import LobbyView Data sets 
clients = pd.read_csv("/content/drive/MyDrive/dataset___client_level.csv")

issues = pd.read_csv("/content/drive/MyDrive/dataset___issue_level.csv")

reports = pd.read_csv("/content/drive/MyDrive/dataset___report_level.csv")

text = pd.read_csv("/content/drive/MyDrive/dataset___issue_text.csv")

#extract lobby reports that mention Trade- Foreign & Domestic as their issue area code

issuestrd = issues.loc[issues['issue_code'] == "TRD"] 

#extract report texts that mention Trade- Foreign & Domestic as their issue area code

texttrd = text.loc[text['issue_code'] == "TRD"] 

#merge all LobbyView datasets on the issue of "TRD"

reporttexttrd = pd.merge(texttrd, reports)

reporttextclienttrd = pd.merge(reporttexttrd, clients)

reporttextclientissuestrd = pd.merge(reporttextclienttrd, issuestrd)

main = reporttextclientissuestrd

"""Preparing and merging LobbyView data sets for analysis

"""

#drop reports with no desciption of the lobbying activity
mainwithtext = main.dropna(subset=['issue_text']).reset_index(drop=True)

#Extract all reports on trade for the years 2018 onwards
trade2018onwards = mainwithtext.loc[mainwithtext['report_year']>=2018].reset_index(drop=True)

#Replace NaN NAICS values with 0
trade2018onwards['primary_naics'] = trade2018onwards['primary_naics'].fillna(0)

#Select a subset of the trade2018onwards dataframe
clientnaics = trade2018onwards[["client_name", "primary_naics"]]

#Select a subset of the trade2018onwards dataframe
allreports2018 = trade2018onwards[["client_uuid", "client_name", "issue_text"]]

#Change all issue text to string type
allreports2018['issue_text'] = allreports2018['issue_text'].apply(str)

#Create a dataframe of all clients who lobbied on trade 2018 onwards and the number of reports they filed in the period
clients2018 = trade2018onwards['client_name'].value_counts().rename_axis('client_name').reset_index(name='numberofreports')

#merge client level data with unique client naics
df3 = clients2018.merge(clientnaics.drop_duplicates(), on="client_name")

#assign naics values for those provided by LobbyView
df3["naics"] = df3["primary_naics"]

#Assign values if client is firm, 1 by default, handcoded to 0 for organisations
df3["if_firm"] = 1

del df3['numberofreports']

#Replicate Figure 1

main1 = main

#NAICS Code 813910 corresponds to industry association; designate associations and indivudal firms on the same basis

main1['firm_lobbying'] = np.where(main1['primary_naics']!= 813910.0, 1, 0)

main1['association_lobbying'] = np.where(main1['primary_naics'] == 813910.0, 1, 0)

#count the number of firms and associations lobbying for each year

main1["individual"] = main1.groupby('report_year', as_index=False)["firm_lobbying"].transform("sum")

main1["associational"] = main1.groupby('report_year', as_index=False)["association_lobbying"].transform("sum")

main1

#Subset of lobbying clients from the main1 dataframe

mainsum = main1[["report_year", "individual", "associational"]]

#Keep only unique client values

mainsum = mainsum.drop_duplicates().reset_index(drop=True)

mainsum

#Sort dataframe by year

mainsum = mainsum.sort_values(by = "report_year", ascending=True)

#Change year to str object
mainsum["report_year"] = mainsum["report_year"].astype("str")

mainsum = mainsum.iloc[0:-1]

mainsum

#Figure 1 in the paper, as on Page 6

fig, ax = plt.subplots(figsize=(15, 5))

ax.plot(mainsum["report_year"], mainsum["individual"], label="individual")

ax.plot(mainsum["report_year"], mainsum["associational"], label="associational")

ax.legend()

plt.show()

#Upload handcoded naics values for lobby clients

updatednaics = pd.read_csv("/content/drive/MyDrive/datafinal.csv")

"""Natural Language Processing

"""

# Run in python console
import nltk; nltk.download('stopwords')

# Run in terminal or command prompt
!python3 -m spacy download en

# Import spacy
import spacy

# Import pyLDAvis
!pip install pyLDAvis

# Commented out IPython magic to ensure Python compatibility.
import re
from pprint import pprint

# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
from gensim.models.coherencemodel import CoherenceModel


# spacy for lemmatization
import spacy

# Plotting tools
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
pyLDAvis.enable_notebook()
import matplotlib.pyplot as plt
# %matplotlib inline

# Enable logging for gensim - optional
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)

import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)

# NLTK Stop words
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from','related', 'regarding', 'regard', 'relating', 'general', 'from', 're', 'issues', 'discussion', 'matter','relation'])

# Convert to list
data = allreports2018.issue_text.values.tolist()

# Remove Emails
data = [re.sub('\S*@\S*\s?', '', sent) for sent in data]

# Remove new line characters
data = [re.sub('\s+', ' ', sent) for sent in data]

# Remove distracting single quotes
data = [re.sub("\'", "", sent) for sent in data]

pprint(data[:1])

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

data_words = list(sent_to_words(data))

print(data_words[:1])

# Build the bigram and trigram models
bigram = gensim.models.Phrases(data_words, min_count=2, threshold=10) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[data_words], threshold=15)  

# Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

# See trigram example
print(trigram_mod[bigram_mod[data_words[0]]])

# Define functions for stopwords, bigrams, trigrams and lemmatization
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out

# Remove Stop Words
data_words_nostops = remove_stopwords(data_words)

# Form Bigrams
data_words_bigrams = make_bigrams(data_words_nostops)

# python3 -m spacy download en
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])

# Do lemmatization keeping only noun, adj, vb, adv
data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'VERB','ADJ', 'ADV'])

print(data_lemmatized[:1])

# Create Dictionary
id2word = corpora.Dictionary(data_lemmatized)

# Create Corpus
texts = data_lemmatized

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]

# View
print(corpus[:1])

# Build LDA model
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=10, 
                                           random_state=45,
                                           iterations=1000,
                                           update_every=1000,
                                           chunksize=7000,
                                           passes=250,
                                           alpha='auto',
                                           per_word_topics=True)

pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

# Compute Perplexity
print('\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# !pip uninstall pandas --y
# !pip install pandas==1.1.5 --y

# Visualize the topics
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)
vis

# %matplotlib inline

# vis = pyLDAvis.gensim_models.prepare(topic_model=lda_model, corpus=corpus, dictionary=id2word)
# pyLDAvis.enable_notebook()
# pyLDAvis.display(vis)

def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each document
    for i, row in enumerate(ldamodel[corpus]):
        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # => dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = ", ".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']

    # Add original text to the end of the output
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)


df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data)

# Format
df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']

# Show
df_dominant_topic.head(10)

#Reproducing Table in the Appendix 

appendix = df_dominant_topic[["Dominant_Topic", "Keywords"]]
appendix = appendix.drop_duplicates().reset_index(drop=True)
appendix = appendix.sort_values(["Dominant_Topic"]).reset_index(drop=True)
appendix["Intermediate_Issue"] = ["Customs/Duty rules", "Support for Argricultural Exports", "Section 203 Investigations", "US-Mexico-Canada FTA", "Trade Legislation", "Product inclusions in FTAs", "FTA renegeotiations", "Bilateral Negotiations", "Tariff Ecxlusion/Exemptions", "National Security"]

appendix

#Merging reports with clients 

df5 = df_dominant_topic
df5["client_name"] = trade2018onwards["client_name"]
df5

naics2018only = updatednaics[["client_name", "naics", "if_firm"]]
naics2018only

#Merging LDA-applied text to client and naics
df5 = df5.merge(naics2018only, on = "client_name")
df5

#Function to count the number of characters in a given object
def count_digits(string):
    return sum(item.isdigit() for item in string)

#Remove decimals from NAICS. alues after onverting them to str type

df5['naics'] = df5['naics'].astype(str).replace('\.0', '', regex=True)

#if_firm dummy variable converted to int  

df5["if_firm"] = df5['if_firm'].astype(int)

#count_digits function applied

df5['digits'] = df5['naics'].apply(count_digits)

#Proportion of firms as lobby clients for each NAIC industry
result = df5.groupby('naics').agg({'if_firm': ['mean']})
result

df5 = df5.merge(result, on = 'naics')

#Compute unique number of Dominant topics for all documents in a NAICS industry

df5['complexity'] = df5.groupby('naics')['Dominant_Topic'].transform('nunique') #count(distinct)

df5

#Import US Census Bureau data for all NAICS industries, including number of firms

industry_emp = pd.read_csv("/content/drive/MyDrive/us_6digitnaics_2016.csv")

industry_emp

new_header = industry_emp.iloc[0] #grab the first row for the header

industry_emp = industry_emp[1:] #take the data less the header row

industry_emp.columns = new_header #set the header row as the df header

#Number of total establishments for the entire industry

a = industry_emp.iloc[0:1, 2:3].values

a

#Apply count_digits to NAICS code in the Census Bureau Dataset

industry_emp["digits"] = industry_emp["Code"].apply(count_digits)

#Subset includes all six-digit NAICS industries

industry_emp_six = industry_emp[industry_emp['digits']==6]

#Subset includes NAICS code, number of establishments by industry, and employment categories

industry_emp_six = industry_emp_six[['Code', 'NUMBER_FIRMS','ET_EMPLOYMENT_SIZE']]

#Select total number of establishments by industry

industry_emp_six = industry_emp_six[industry_emp_six["ET_EMPLOYMENT_SIZE"] == "01:  Total" ]

#Rename "code" column to "naics" for compatibility with other datasets

industry_emp_six = industry_emp_six.rename(columns = {"Code":"naics"})

#Remove commas

industry_emp_six = industry_emp_six.replace(',',"")

# Assign new column as the one containing proportion of all lobbying clients that are firms\

df5['firm_prpn'] = df5.iloc[:,-2]

#Drop all observations where NAICS value is 0 

df5= df5[df5['naics'] != "0"].reset_index()

#Remove all all observations where naics value contains letters

df5 = df5[~df5['naics'].str.contains("[a-zA-Z]").fillna(False)]

df5['lobby_units'] = df5.groupby('naics')['client_name'].transform('nunique') #count(distinct)

df5['naics'].value_counts()

#Drop all NAICS industries with less than 2 lobby units

df5 = df5[df5["lobby_units"]>=2]

df5["naics"].value_counts()

df5

#Merge with Census Bureau data 

finaldataset = df5.merge(industry_emp_six, on = 'naics').reset_index(drop = True)

finaldataset

#Select subset that includes NAICS code, proportion of lobby clients made up of individual firms, complexity score and number of establishments by industry
finaldataset1 = finaldataset[["naics", 'complexity', 'firm_prpn', 'NUMBER_FIRMS']]
finaldataset1

#Remove commas
finaldataset1['NUMBER_FIRMS'] = finaldataset1['NUMBER_FIRMS'].astype(str).replace(',', '', regex=True)

#Change Number of firms to int type
finaldataset1['NUMBER_FIRMS'] = finaldataset1['NUMBER_FIRMS'].astype(int)

finaldataset1

#Drop duplicate entries
finaldataset1 = finaldataset1.drop_duplicates().reset_index()
finaldataset1

#Save to csv file
finaldataset1.to_csv('finaldataset1_.csv')
!cp finaldataset1_.csv "drive/My Drive/"

#Replicate Figure 3
doc_lens = [len(d) for d in df_dominant_topic.Text]

# Plot
plt.figure(figsize=(16,7), dpi=160)
plt.hist(doc_lens, bins = 1000, color='navy')
plt.text(900, 120, "Mean   : " + str(round(np.mean(doc_lens))))

plt.text(900,  80, "Median : " + str(round(np.median(doc_lens))))
plt.text(900,  40, "Stdev   : " + str(round(np.std(doc_lens))))
# plt.text(750,  70, "1%ile    : " + str(round(np.quantile(doc_lens, q=0.01))))
# plt.text(750,  60, "99%ile  : " + str(round(np.quantile(doc_lens, q=0.99))))

plt.gca().set(xlim=(0, 1000), ylabel='Number of Documents', xlabel='Document Word Count')
plt.tick_params(size=16)
plt.xticks(np.linspace(0,1000,9))
plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))
plt.show()